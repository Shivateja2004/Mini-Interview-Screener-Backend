# ğŸ§  Mini AI Interview Screener (Backend Only)

A lightweight backend API that evaluates candidate answers using an LLM and ranks applicants based on performance.  
Built as part of the **48-Hour Engineering Assignment â€” VantaHire | AI Revo Labs**.

---

## ğŸš€ Features

| Feature | Status |
|---|---|
| Evaluate candidate answer using LLM | âœ” |
| Score (1â€“5) + summary + improvement feedback | âœ” |
| Rank multiple applicants based on score | âœ” |
| Clean JSON responses | âœ” |
| Fully testable via Swagger UI | âœ” |

---

## ğŸ›  Tech Stack + Why Chosen

| Technology | Purpose | Why chosen |
|---|---|---|
| **Python** | Core backend | Fast development & clean logic |
| **FastAPI** | API framework | Auto docs, async, modern, scalable |
| **Groq LLM API** | AI evaluation | Fast inference, structured output |
| **Uvicorn** | ASGI server | Lightweight and production-ready |

> Decision Thinking:  
FastAPI helped deliver functionality quickly with zero UI requirement.  
Groq LLM ensured instant responses without token billing concerns.
Python gave flexibility to implement ranking logic reliably under time pressure.

---

## ğŸ“‚ Project Structure

```
mini-ai-interview-screener/
â”‚â”€â”€ app.py                # Main API application
â”‚â”€â”€ requirements.txt      # Dependencies list
â”‚â”€â”€ .env                  # Keys/Config (not pushed to GitHub)
â”‚â”€â”€ README.md             # Documentation
```

---

## ğŸ’¾ Installation & Setup

### 1ï¸âƒ£ Clone the project

```bash
git clone <your-repo-url>
cd mini-ai-interview-screener
```

### 2ï¸âƒ£ Create & activate virtual environment

```bash
python -m venv venv
.\venv\Scripts\activate   # Windows
```

### 3ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Create **.env** file (required)

```
GROQ_API_KEY='API'
MODEL=llama-3.1-8b-instant
```

### 5ï¸âƒ£ Run Server

```bash
uvicorn app:app --reload
```

### Access API Docs

```
http://127.0.0.1:8000/docs
```

---

## ğŸ”¥ API Endpoints

### 1ï¸âƒ£ /evaluate-answer

**POST Body:**

```json
{
  "answer": "I would design microservices with caching & load balancing."
}
```

**Sample Output:**

```json
{
  "score": 4,
  "summary": "Good understanding of scalable systems",
  "improvement": "Explain deeper implementation strategy"
}
```

---

### 2ï¸âƒ£ /rank-candidates

**POST Body:**

```json
{
  "answers": [
    "I design scalable services.",
    "I use performance optimization.",
    "I build modular backend systems."
  ]
}
```

**Output Example:**

```json
{
  "ranked_candidates": [
    { "answer": "...", "score": 5, "summary": "...", "improvement": "..."},
    { "answer": "...", "score": 4, "summary": "...", "improvement": "..."},
    { "answer": "...", "score": 3, "summary": "...", "improvement": "..."}
  ]
}
```

---

## ğŸ“œ Evaluation Criteria Covered

| Requirement | Completed |
|---|---|
| Integrate LLM | âœ” |
| Score + Evaluate Answers | âœ” |
| Ranking Function | âœ” |
| JSON Responses | âœ” |
| Clean API Structure | âœ” |
| Documentation + Setup | âœ” |

---

